<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Workshop Schedule - Day 1</title>
</head>
<body>
    <table>
        <thead>
            <tr>
                <th width="15%">Timeslot</th>
                <th>Speaker</th>
                <th width="65%">Schedule/Topic of the Talk</th>
            </tr>
        </thead>
        <tbody>
            <tr class="double-line session-header">
                <td>08:00 - 14:00</td>
                <td colspan="2"><b>Get to 강릉 & Lunch & etc <br>(<b>Lunch:</b> 강문어화횟집 (물회)) </b></td>
                <td></td>
            </tr>
            <tr class="double-line session-header">
                <td colspan="3"><b>Special Talks by Prof's</b></td>
            </tr>
            <tr>
                <td>14:30 - 15:00</td>
                <td><b><a href="https://minoh.io/" target="_blank">Min-hwan Oh</a></b><br>Assistant Professor<br><a href="https://gsds.snu.ac.kr/" target="_blank">SNU DS</a></td>
                <td><h3>TBD</h3>
                    <!-- <a href="files/minhwan-oh.pdf" target="blank">Slides</a> -->
                </td>
            </tr>
            <tr>
                <td>15:00 - 15:10</td>
                <td><b>Short Break</b></td>
                <td></td>
            </tr>
            <tr>
                <td>15:10 - 15:40</td>
                <td><b><a href="https://ernestryu.com/" target="_blank">Ernest K. Ryu</a></b><br>Assistant Professor<br><a href="https://ww3.math.ucla.edu/" target="_blank">UCLA Math</a></td>
                <td><h3>TBD</h3>
                    <!-- <a href="files/yeoneung-kim.pdf" target="blank">Slides</a> -->
                </td>
            </tr>
            <tr>
                <td>15:40 - 15:50</td>
                <td><b>Short Break</b></td>
                <td></td>
            </tr>
            <tr>
                <td>15:50 - 16:20</td>
                <td><b><a href="https://chulheeyun.github.io/" target="_blank">Chulhee "Charlie" Yun</a></b><br>Assistant Professor<br><a href="https://gsai.kaist.ac.kr/" target="_blank">KAIST AI</a></td>
                <td><h3>TBD</h3>
                    <!-- <a href="files/yeoneung-kim.pdf" target="blank">Slides</a> -->
                </td>
            </tr>
            <tr>
                <td>16:20 - 16:30</td>
                <td><b>Short Break</b></td>
                <td></td>
            </tr>
            <tr class="double-line session-header">
                <td>16:30 - 18:00</td>
                <!-- <td colspan="2"><b>Lunchtime + Coffee + Free Discussions</b><br>(<b>Lunch:</b> 교직원식당 @KAIST Seoul Campus — <i>예약확정</i>)</td> -->
                <td colspan="3"><b>Poster Session #1</b></td>
            </tr>
            <tr class="double-line session-header">
                <td>18:00 - </td>
                <td colspan="2"><b><b>Dinner:</b> 13월의 강릉 - 강릉본점 (조개구이 패밀리)</b></td>
            </tr>
        </tbody>
    </table>

    <br>
    <h1>Special Talks by Prof's</h1>
            <div class="session-box">
            <div class="session-box">
                <div class="session-info">
                    <h3>Min-hwan Oh: TBD </h3>
                </div>
                <div class="abstract">
                    <h4>Abstract:</h4>
                    <p>TBD</p>
                </div>
                <div class="speaker-bio">
                    <h4>Speaker Bio:</h4>
                    <p> Min-hwan Oh is an Assistant Professor in the Graduate School of Data Science at Seoul National University. 
                        His research interests are in sequential decision making under uncertainty including bandit algorithms and reinforcement learning, statistical machine learning, and optimization. </p>
                </div>
            </div>

            <div class="session-box">
                <div class="session-info">
                    <h3>Ernest K. Ryu: TBD</h3>
                </div>
                <div class="abstract">
                    <h4>Abstract:</h4>
                    <p>TBD</p>
                </div>
                <div class="speaker-bio">
                    <h4>Speaker Bio:</h4>
                    <p> Ernest Ryu is an Assistant Professor in the Department of Mathematics at UCLA. His current research focus is on applied mathematics, deep learning, and optimization.
                        Professor Ryu received a B.S. degree in Physics and Electrical engineering with honors at the California Institute of Technology in 2010 and an M.S. in Statistics and a Ph.D. in Computational and Mathematical Engineering with the Gene Golub Best Thesis Award at Stanford University in 2016. In 2016, he joined the Department of Mathematics at UCLA, as an Assistant Adjunct Professor. In 2020, he joined the Department of Mathematical Sciences at Seoul National University as a tenure-track faculty. In 2024, returned to UCLA as an assistant professor. </p>
                </div>

            </div>

            <div class="session-box">
                <div class="session-info">
                    <h3>Chulhee "Charlie" Yun: TBD</h3>
                </div>
                <div class="abstract">
                    <h4>Abstract:</h4>
                    <p>TBD</p>
                </div>
                <div class="speaker-bio">
                    <h4>Speaker Bio:</h4>
                    <p> Chulhee “Charlie” Yun is an assistant professor at KAIST Kim Jaechul Graduate School of AI, where he directs the Optimization & Machine Learning Laboratory. He finished his PhD from the Laboratory for Information and Decision Systems and the Department of Electrical Engineering & Computer Science at MIT, under the joint supervision of Prof. Suvrit Sra and Prof. Ali Jadbabaie. Charlie’s research spans optimization, machine learning theory, and deep learning theory, with the driving goal of bridging the gap between theory and practice in modern machine learning. </p>
                </div>

            </div>
    </div>
    <br>
    <br>
    <h1>Poster Session #1</h1>
    <div class="session-box">
        <div class="poster">
            <h3 class="poster-title">Multi-Step Offline Reinforcement Learning with Peng's Q($\lambda$) via Gaussian Mixture Model</h3>
            <button class="toggle-abstract show">Show Abstract</button>
            <div class="poster-abstract">
                <h4>Authors:</h4>
                <p>Byeongchan Kim and Min-hwan Oh</p>
                <h4>Abstract:</h4>
                <p>
                    In this paper, we propose multi-step offline Q-learning that effectively adapts forward-looking multi-step updates of value functions, named \texttt{MOQL}. 
                    Our proposed method adapts Peng's Q($\lambda$) operator to offline Reinforcement Learning (RL) based on the distribution of average rewards of each offline trajectory. 
                    The distribution of average rewards is estimated using a Gaussian mixture model, and offline trajectories clustered by each Gaussian component are assigned to 
                    their respective adapted $\lambda$ instead of a fixed $\lambda$. 
                    To the best of our knowledge, we first show the effectiveness of an off-policy multi-step offline operator that fully exploits previously realized rewards in offline trajectories. 
                    Through extensive numerical experiments, we validate that offline RL algorithms incorporating our proposed multi-step offline operator significantly outperform single-step 
                    offline RL algorithms in benchmark datasets.
                </p>
            </div>
        </div>
        <div class="poster">
            <h3 class="poster-title">Coordinated Exploration in Distributed Reinforcement Learning</h3>
            <button class="toggle-abstract show">Show Abstract</button>
            <div class="poster-abstract">
                <h4>Authors:</h4>
                <p>Kyonghyun Min and Min-hwan Oh</p>
                <h4>Abstract:</h4>
                <p>
                    We propose Collective Optimism, a new method for Coordinated Exploration in Distributed Reinforcement Learning. 
                    The primary objective of collective optimism is to efficiently coordinate agents' exploration strategies, a concept that has not been previously considered in existing models. 
                    This algorithm ensures the maximization of overall performance by taking into account the uncertainty of the current state-action pair, without relying on randomness. 
                    A strength of this method is its remarkable ability to be highly applicable to any RL model. 
                    To demonstrate the validity of our method and to show its adoptability, we implemented and evaluated the performance of CODRL, model through a series of numerical experiments.
                </p>
            </div>
        </div>
        <div class="poster">
            <h3 class="poster-title">Doubly Robust Parametric Bandit Model for Partially Observable Features</h3>
            <button class="toggle-abstract show">Show Abstract</button>
            <div class="poster-abstract">
                <h4>Authors:</h4>
                <p>Wonyoung Kim, Sungwoo Park, Garud Iyengar, Assaf Zeevi, and Min-hwan Oh</p>
                <h4>Abstract:</h4>
                <p>
                    We introduce a novel parametric bandit problem where the rewards are determined by the feature vectors and a subset of features may not be observable. 
                    While the unobserved features lead the agent to use spurious estimate that causes regret linear to the decision epoch, tackling unobserved features is challenging 
                    since their information (e.g., dimension, relationship with rewards or observed features) is not available. 
                    We propose a novel algorithm that achieves a regret bound sublinear in horizon $T$ without any knowledge of the unobserved features. 
                    Specifically, the proposed algorithm achieves $\tilde{O}(\sqrt{(d + d_h)T})$, where $d$ represents the dimension of observable features, 
                    and $d_h$ is the \emph{unknown} dimension of the unobserved feature space projected onto the reward space. 
                    The proposed algorithm is agnostic to~$d_h$, which is $0$ when all features are observable and tends to increase to $K-d$ as the number of unobserved features increases, 
                    where $K$ is the number of arms. The core of the algorithm is in (i) augmenting basis vectors that are orthogonal to the observable feature space, 
                    and (ii) employing the doubly-robust estimator for the rewards on the augmented basis vectors that converges with $\tilde{O}(t^{-1/2})$-rate on all $K$ arms in every round $t$. 
                    Our numerical experiments validate that our proposed algorithm outperforms both previous non-contextual multi-armed bandit and linear bandit algorithms.
                </p>
            </div>
        </div>
        <div class="poster">
            <h3 class="poster-title">Task Diversity Shortens the Plateau Between No-Context Learning & In-Context Learning</h3>
            <button class="toggle-abstract show">Show Abstract</button>
            <div class="poster-abstract">
                <h4>Presenter:</h4>
                <p>
                    <a href="https://lthilnklover.github.io/" target="_blank">Joo Young Choi</a>
                </p>
                <h4>Abstract:</h4>
                <p>
                    In-context learning (ICL) refers to a model's ability to condition on a prompt sequence consisting of in-context
                    examples (input-output pairs from a task) along with a new query input, and generate the corresponding output.
                    High performance in ICL has been reported for large language models, prompting efforts to understand its
                    underlying mechanisms. Garg et al. (2022) studied transformers' ability to learn simple functions in-context by
                    training GPT-2 from scratch, leading to numerous follow-up studies. Interestingly, several of these studies
                    observed loss plateaus in the initial phase of training. These plateaus, however, have not been characterized or
                    considered as training failures. In this work, through experiments on various tasks, we demonstrate that these
                    plateaus are no-context learning regime: transformers learn the optimal context-free function during plateaus.
                    Furthermore, when multiple ICL tasks are mixed, these plateaus are shortened. This finding challenges the
                    common belief that increased task diversity complicates the training process.
                    <br><br>
                    Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes." In NeurIPS 2022.
                </p>
            </div>
        </div>
        <div class="poster">
            <h3 class="poster-title">Fine-tuning language model under homomorphic encryption</h3>
            <button class="toggle-abstract show">Show Abstract</button>
            <div class="poster-abstract">
                <h4>Presenter:</h4>
                <p>
                    Donghwan Rho
                </p>
                <h4>Abstract:</h4>
                <p>
                    There are many privacy-sensitive data, such as finance, clinical data, etc. To use these data in machine learning,
                    we should preprocess these data to delete private information. But this is too costly. To overcome this issue,
                    if we use homomorphic encryption (HE), we can use privacy-sensitive data in machine learning. But HE is too
                    slow to implement. One of the reasons is the softmax operation. Another reanson is fine-tuning, since it
                    requires too many multiplications of ciphertext matrices. To resolve the first issue, we use Gaussian Kernel to
                    implement division-free attention. Also, to settle the second issue, we show that using LoRA convert ciphertext-
                    ciphertext matrix multiplications into ciphertext-plaintext maxtrix multiplications. In this way, using LoRA can
                    reduce fine-tuning time in the HE setting even more than in plaintext. With these methods, we can significantly
                    reduce the time implementing transformer-based language model under HE.
                </p>
            </div>
        </div>
        <div class="poster">
            <h3 class="poster-title">TBD</h3>
            <button class="toggle-abstract show">Show Abstract</button>
            <div class="poster-abstract">
                <h4>Presenter:</h4>
                <p>
                    TBD
                </p>
                <h4>Abstract:</h4>
                <p>TBD</p>
            </div>
        </div>
        <div class="poster">
            <h3 class="poster-title">
                <a href="https://openreview.net/forum?id=GR5LXaglgG" target="_blank">DASH: Warm-Starting Neural Network Training Without Loss of Plasticity Under Stationarity</a>
            </h3>
            <button class="toggle-abstract show">Show Abstract</button>
            <div class="poster-abstract">
                <h4>Authors:</h4>
                <p>Baekrok Shin*, Junsoo Oh*, Hanseul Cho, and Chulhee Yun</p>
                <h4>Venue:</h4>
                <p><a href="https://want-ai-hpc.github.io/icml2024/about/">ICML '24 WANT Workshop</a></p>
                <h4>Abstract:</h4>
                <p>
                    Warm-starting neural networks by initializing them with previously learned weights is appealing, 
                    as practical neural networks are often deployed under a continuous influx of new data. 
                    However, it often leads to loss of plasticity, where the network loses its ability to learn new information, resulting in worse generalization compared to training from scratch. 
                    This occurs even under stationary data distributions, and its underlying mechanism is poorly understood. 
                    We develop a framework emulating real-world neural network training and identify noise memorization as the primary cause of plasticity loss when warm-starting on stationary data. 
                    Motivated by this, we propose <b>Direction-Aware SHrinking (DASH)</b>, a method aiming to mitigate plasticity loss by selectively forgetting memorized noise 
                    while preserving learned features. We validate our approach on vision tasks, demonstrating improvements in test accuracy and training efficiency.
                </p>
            </div>
        </div>
        <div class="poster">
            <h3 class="poster-title"><a href="https://openreview.net/forum?id=TW3ipYdDQG" target="_blank">Fair Streaming Principal Component Analysis: Statistical and Algorithmic Viewpoint</a></h3>
            <button class="toggle-abstract show">Show Abstract</button>
            <div class="poster-abstract">
                <h4>Authors:</h4>
                <p>Junghyun Lee*, Hanseul Cho*, Se-Young Yun, and Chulhee Yun</p>
                <h4>Venue:</h4>
                <p>NeurIPS '23</p>
                <h4>Abstract:</h4>
                <p>
                    Fair Principal Component Analysis (PCA) is a problem setting where we aim to perform PCA while making the resulting representation fair in that the projected distributions, 
                    conditional on the sensitive attributes, match one another. 
                    However, existing approaches to fair PCA have two main problems: theoretically, there has been no statistical foundation of fair PCA in terms of learnability; 
                    practically, limited memory prevents us from using existing approaches, as they explicitly rely on full access to the entire data. 
                    On the theoretical side, we rigorously formulate fair PCA using a new notion called probably approximately fair and optimal (PAFO) learnability. 
                    On the practical side, motivated by recent advances in streaming algorithms for addressing memory limitation, we propose a new setting called fair streaming PCA 
                    along with a memory-efficient algorithm, fair noisy power method (FNPM). 
                    We then provide its statistical guarantee in terms of PAFO-learnability, which is the first of its kind in fair PCA literature. 
                    We verify our algorithm in the CelebA dataset without any pre-processing; while the existing approaches are inapplicable due to memory limitations, 
                    by turning it into a streaming setting, we show that our algorithm performs fair PCA efficiently and effectively.
                </p>
            </div>
        </div>
        <div class="poster">
            <h3 class="poster-title"><a href="https://arxiv.org/abs/2311.15051" target="_blank">Gradient Descent with Polyak's Momentum Finds Flatter Minima via Large Catapults</a></h3>
            <button class="toggle-abstract show">Show Abstract</button>
            <div class="poster-abstract">
                <h4>Authors:</h4>
                <p>Prin Phunyaphibarn*, Junghyun Lee*, Bohan Wang, Huishuai Zhang, and Chulhee Yun</p>
                <h4>Venue:</h4>
                <p><a href="https://sites.google.com/view/hidimlearning/home">ICML '24 HiLD Workshop</a> & <a href="https://sites.google.com/view/m3l-2023/home?authuser=0">NeurIPS '23 M3L Workshop <b>Oral</b></a></p>
                <h4>Abstract:</h4>
                <p>
                    Although gradient descent with Polyak's momentum is widely used in modern machine and deep learning, a concrete understanding of its effects on the training trajectory remains elusive. 
                    In this work, we empirically show that for linear diagonal networks and nonlinear neural networks, momentum gradient descent with a large learning rate displays large catapults, 
                    driving the iterates towards much flatter minima than those found by gradient descent. 
                    We hypothesize that the large catapult is caused by momentum "prolonging" the self-stabilization effect (Damian et al., 2023). 
                    We provide theoretical and empirical support for our hypothesis in a simple toy example and empirical evidence supporting our hypothesis for linear diagonal networks.
                </p>
            </div>
        </div>
    </div>
    <br>
</body>
</html>